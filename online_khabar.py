# -*- coding: utf-8 -*-
"""online_khabar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14QpznlcNz-GbteaAJtA0niV1tz9CA5Do
"""

import csv
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import pandas as pd
import numpy as np
import spacy

def extract_all_links(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)

        # Check if the request was successful (status code 200)
        if response.status_code == 200:
            # Parse the HTML content of the page
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract all links using BeautifulSoup methods
            links = [a['href'] for a in soup.find_all('a', href=True)]

            # Convert relative URLs to absolute URLs
            links = [urljoin(url, link) for link in links]

            return links
        else:
            print(f"Failed to retrieve data from '{url}'. Status code: {response.status_code}")
            return []
    except requests.exceptions.RequestException as e:
        print(f"Error connecting to {url}: {e}")
        return []

def save_links_to_csv(links, csv_filename):
    with open(csv_filename, 'w', newline='') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(['Link'])  # Write header

        for link in links:
            csv_writer.writerow([link])


url = 'https://english.onlinekhabar.com/'
links = extract_all_links(url)
print(links)
# Save links to CSV file
csv_filename = 'extracted_links1.csv'
save_links_to_csv(links, csv_filename)

# Print a message indicating success
print(f"All links saved to {csv_filename}")
def is_internal_link(base_url, link):
    # Check if the link is internal to the base_url domain
    base_domain = urlparse(base_url).netloc
    link_domain = urlparse(link).netloc

    return base_domain == link_domain

def extract_all_links(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)

        # Check if the request was successful (status code 200)
        if response.status_code == 200:
            # Parse the HTML content of the page
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract all links using BeautifulSoup methods
            links = [a['href'] for a in soup.find_all('a', href=True)]

            # Convert relative URLs to absolute URLs
            links = [urljoin(url, link) for link in links]

            return links
        else:
            print(f"Failed to retrieve data from '{url}'. Status code: {response.status_code}")
            return []
    except requests.exceptions.RequestException as e:
        print(f"Error connecting to {url}: {e}")
        return []

def save_links_to_csv(links, csv_filename):
    with open(csv_filename, 'w', newline='') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(['Link', 'Type'])  # Write header

        for link in links:
            link_type = 'Internal' if is_internal_link(url, link) else 'External'
            csv_writer.writerow([link, link_type])

# Specify the URL
url = 'https://english.onlinekhabar.com/'

# Extract all links from the specified URL
links = extract_all_links(url)

print(links)

# Save links to CSV file with their types (Internal or External)
csv_filename = 'extracted_links1.csv'
save_links_to_csv(links, csv_filename)

# Print a message indicating success
print(f"All links saved to {csv_filename}")

link=pd.read_csv('extracted_links1.csv')

def is_social_media_url(url):
    social_media_domains = ['twitter.com', 'facebook.com', 'instagram.com']  # Add more if needed
    for domain in social_media_domains:
        if domain in url:
            return True
    return False

def extract_specific_content(url):
    try:
        response = requests.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Skip extraction for social media URLs
        if is_social_media_url(url):
            print(f"Skipping {url} because it's a social media link.")
            return None, None, None, None, None  # Include the original link as None

        heading_element = soup.find('div', class_='ok-post-header')
        #h1_text = heading_element.find('h1').text.strip()
        author_element = soup.find('div', class_='ok-author flx')
        publication_date_element = soup.find('div', class_='ok-author flx')
        content_container = soup.find('div', class_='post-content-wrap')

        heading = heading_element.find('h1').text.strip() if heading_element else 'Heading not found'
        author = author_element.find('img')['alt'] if author_element else 'Author not found'
        publication_date_raw = publication_date_element.find('span', class_='ok-post-date').text if publication_date_element else 'Date not found'  # Corrected variable name
        # Remove the unwanted prefix "प्रकाशित :"
        publication_date = publication_date_raw.replace('Published Date:', '').strip()
        content1 = content_container.find_all('p') if content_container else 'Content not found'
        content = ' '.join([str(p) for p in content1])

        return heading, author, publication_date, content, url  # Include the original link
    except requests.exceptions.RequestException as e:
        print(f"Failed to retrieve content from {url}. Error: {e}")
        return None, None, None, None, url  # Include the original link

def save_to_csv(data, csv_file_path):
    with open(csv_file_path, 'a', newline='', encoding='utf-8') as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(['Heading', 'Author', 'Publication_date', 'Source', 'Content', 'Link'])  # Updated header
        csv_writer.writerow(data)  # Added 'ekantipur' as the source

def main():
    csv_file_path = 'extracted_data1.csv'

    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(['Heading', 'Author', 'Publication_date','Source','Content' ,'Link'])  # Updated header

    with open('extracted_links1.csv', 'r') as csv_file:
        reader = csv.DictReader(csv_file)
        urls = [row['Link'] for row in reader]

    for url in urls:
        if not urlparse(url).scheme:
            url = urljoin('https://', url)

        if urlparse(url).netloc and not is_social_media_url(url):
            heading, author, publication_date, content, link = extract_specific_content(url)
            if heading is not None and author is not None and content is not None:
                data_to_save = [heading, author, publication_date,'english_online_khabar',content, link]
                print(f"Data for {url}:\nHeading: {heading}\nAuthor: {author}\nPublication Date: {publication_date}\nSource: english_online_khabar\nContent: {content}\nLink: {link}\n")
                save_to_csv(data_to_save, csv_file_path)
                print(f"Data saved for {url}")
        else:
            print(f"Invalid URL format or social media link: {url}")

if __name__ == "__main__":
    main()


data = pd.read_csv('extracted_data1.csv')

df = pd.read_csv('extracted_data1.csv')

df_no_duplicates = df.drop_duplicates()

df_no_duplicates.to_csv('Filtered_data.csv', index=False)

df = pd.read_csv('extracted_data1.csv')  # Replace 'your_file.csv' with the actual file path

# Drop duplicates based on all columns
df_no_duplicates = df.drop_duplicates()

# Save the DataFrame without duplicates back to a CSV file
df_no_duplicates.to_csv('Filtered_data.csv', index=False)

data=pd.read_csv('Filtered_data.csv')
#data

df = pd.read_csv('Filtered_data.csv')

# Remove rows where "Heading" or "Content" contains "Heading not found" or "Content not found"
df_filtered = df[~df['Heading'].str.contains('Heading not found') & ~df['Content'].str.contains('Content not found')]

# Save the filtered DataFrame to a new CSV file
df_filtered.to_csv('Filtered_data.csv', index=False, sep=';')


#print(df_filtered)



nlp = spacy.load("en_core_web_sm")


data = pd.read_csv('Filtered_data.csv', sep=';')


def classify_text(Content):
    doc = nlp(Content)


    categories = {
        "HOME": ["home", "welcome", "main"],
        "KATHMANDU": ["kathmandu"],
        "NEPAL": ["nepal"],
        "COVID-19": ["Covid19"],
        "COVID CONNECT": ["covid connect", "covid updates"],
        "WORLD": ["world", "international"],
        "OPINION": ["opinion", "editorial"],
        "BUSINESS": ["business", "economy"],
        "SPORTS": ["sports", "athletics"],
        "ENTERTAINMENT": ["entertainment", "movies", "music"],
        "LIFESTYLE": ["lifestyle"],
        "SCIENCE&TECH": ["science", "tech", "technology"],
        "BLOG": ["blog", "personal"],
        "ENVIRONMENT": ["environment", "nature"],
        "HEALTH": ["health", "wellness"],
    }

    # Check for category keywords in the processed text
    for category, keywords in categories.items():
        for keyword in keywords:
            if keyword in Content.lower():
                return category

    # If no category is found, return None
    return None

# Replace NaN values with an empty string in the 'Content' column
data['Content'] = data['Content'].replace(np.nan, '')

# Apply the classification function to each row in the 'Content' column
data['Category'] = data['Content'].apply(classify_text)

# Save the DataFrame to a new CSV file
data.to_csv('classified_data_seto_pati.csv', index=False, sep=';')

# Print the DataFrame with the classified categories
#print(data)

data.tail(100)





